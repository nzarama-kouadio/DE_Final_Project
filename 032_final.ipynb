{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Model for Fraud Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "# Model Selection and Cross-Validation\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    KFold,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    ")\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "# Imbalanced Data Handling\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TransactionID', 'Timestamp', 'MerchantID', 'Amount', 'CustomerID',\n",
       "       'TransactionAmount', 'AnomalyScore', 'FraudIndicator', 'Category',\n",
       "       'MerchantName', 'MerchantLocation', 'CustomerName', 'CustomerAge',\n",
       "       'CustomerAddress', 'AccountBalance', 'LastLogin', 'SuspiciousFlag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud = pd.read_csv(\"complete_dataset.csv\")\n",
    "fraud.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_be_dropped = [\n",
    "    \"TransactionID\",\n",
    "    \"MerchantID\",\n",
    "    \"CustomerID\",\n",
    "    \"CustomerName\",\n",
    "    \"MerchantName\",\n",
    "    \"MerchantLocation\",\n",
    "    \"CustomerAddress\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Timestamp', 'Amount', 'TransactionAmount', 'AnomalyScore',\n",
       "       'FraudIndicator', 'Category', 'CustomerAge', 'AccountBalance',\n",
       "       'LastLogin', 'SuspiciousFlag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud1 = fraud.drop(columns_to_be_dropped, axis=1)\n",
    "fraud1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FraudIndicator\n",
       "0    955\n",
       "1     45\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud1[\"FraudIndicator\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is very imbalanced as the number of cases which are fraudulent are very few. Thus, the models would not be able to predict these cases very accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp            datetime64[ns]\n",
      "Amount                      float64\n",
      "TransactionAmount           float64\n",
      "AnomalyScore                float64\n",
      "FraudIndicator                int64\n",
      "Category                     object\n",
      "CustomerAge                   int64\n",
      "AccountBalance              float64\n",
      "LastLogin            datetime64[ns]\n",
      "SuspiciousFlag                int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# converting the TimeStamp to a datetime format\n",
    "fraud1[\"Timestamp\"] = pd.to_datetime(fraud1[\"Timestamp\"])\n",
    "fraud1[\"LastLogin\"] = pd.to_datetime(fraud1[\"LastLogin\"])\n",
    "print(fraud1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud1[\"gap\"] = (fraud1[\"Timestamp\"] - fraud1[\"LastLogin\"]).dt.days.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract useful time-based features\n",
    "fraud1[\"Hour\"] = fraud1[\"Timestamp\"].dt.hour\n",
    "fraud1[\"Day\"] = fraud1[\"Timestamp\"].dt.day\n",
    "fraud1[\"Month\"] = fraud1[\"Timestamp\"].dt.month\n",
    "fraud1[\"Weekday\"] = fraud1[\"Timestamp\"].dt.weekday\n",
    "fraud1[\"Year\"] = fraud1[\"Timestamp\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fraud1.drop([\"FraudIndicator\", \"LastLogin\", \"Timestamp\"], axis=1)\n",
    "y = fraud1[\"FraudIndicator\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amount</th>\n",
       "      <th>TransactionAmount</th>\n",
       "      <th>AnomalyScore</th>\n",
       "      <th>Category</th>\n",
       "      <th>CustomerAge</th>\n",
       "      <th>AccountBalance</th>\n",
       "      <th>SuspiciousFlag</th>\n",
       "      <th>gap</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55.530334</td>\n",
       "      <td>79.413607</td>\n",
       "      <td>0.686699</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>2869.689912</td>\n",
       "      <td>0</td>\n",
       "      <td>951</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.881180</td>\n",
       "      <td>12.053087</td>\n",
       "      <td>0.081749</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>9527.947107</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50.176322</td>\n",
       "      <td>33.310357</td>\n",
       "      <td>0.023857</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>9288.355525</td>\n",
       "      <td>0</td>\n",
       "      <td>954</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.634001</td>\n",
       "      <td>46.121117</td>\n",
       "      <td>0.876994</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>5588.049942</td>\n",
       "      <td>0</td>\n",
       "      <td>795</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78.122853</td>\n",
       "      <td>54.051618</td>\n",
       "      <td>0.034059</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>7324.785332</td>\n",
       "      <td>0</td>\n",
       "      <td>945</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>86.947084</td>\n",
       "      <td>34.545138</td>\n",
       "      <td>0.121173</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>3152.247787</td>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>51.147096</td>\n",
       "      <td>55.383113</td>\n",
       "      <td>0.109892</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>9253.478917</td>\n",
       "      <td>0</td>\n",
       "      <td>310</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>56.163984</td>\n",
       "      <td>17.855878</td>\n",
       "      <td>0.780534</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>8765.035861</td>\n",
       "      <td>0</td>\n",
       "      <td>692</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>37.182412</td>\n",
       "      <td>75.659944</td>\n",
       "      <td>0.010471</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>8935.007146</td>\n",
       "      <td>0</td>\n",
       "      <td>346</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17.245409</td>\n",
       "      <td>67.931879</td>\n",
       "      <td>0.029376</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>5541.197921</td>\n",
       "      <td>0</td>\n",
       "      <td>573</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Amount  TransactionAmount  AnomalyScore  Category  CustomerAge  \\\n",
       "0  55.530334          79.413607      0.686699         2           50   \n",
       "1  12.881180          12.053087      0.081749         1           46   \n",
       "2  50.176322          33.310357      0.023857         4           34   \n",
       "3  41.634001          46.121117      0.876994         4           33   \n",
       "4  78.122853          54.051618      0.034059         2           18   \n",
       "5  86.947084          34.545138      0.121173         0           45   \n",
       "6  51.147096          55.383113      0.109892         0           25   \n",
       "7  56.163984          17.855878      0.780534         4           27   \n",
       "8  37.182412          75.659944      0.010471         3           20   \n",
       "9  17.245409          67.931879      0.029376         2           55   \n",
       "\n",
       "   AccountBalance  SuspiciousFlag  gap  Hour  Day  Month  Weekday  Year  \n",
       "0     2869.689912               0  951     0    1      1        5  2022  \n",
       "1     9527.947107               0   26     1    1      1        5  2022  \n",
       "2     9288.355525               0  954     2    1      1        5  2022  \n",
       "3     5588.049942               0  795     3    1      1        5  2022  \n",
       "4     7324.785332               0  945     4    1      1        5  2022  \n",
       "5     3152.247787               0  203     5    1      1        5  2022  \n",
       "6     9253.478917               0  310     6    1      1        5  2022  \n",
       "7     8765.035861               0  692     7    1      1        5  2022  \n",
       "8     8935.007146               0  346     8    1      1        5  2022  \n",
       "9     5541.197921               0  573     9    1      1        5  2022  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# fit and transform the Category column\n",
    "X[\"Category\"] = label_encoder.fit_transform(X[\"Category\"])\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 13), (200,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the sizes\n",
    "X_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       192\n",
      "           1       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.96       200\n",
      "   macro avg       0.48      0.50      0.49       200\n",
      "weighted avg       0.92      0.96      0.94       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nrutachoudhari/opt/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/nrutachoudhari/opt/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/nrutachoudhari/opt/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/nrutachoudhari/opt/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "log_mod = LogisticRegression()\n",
    "\n",
    "log_mod.fit(X_train, y_train)\n",
    "\n",
    "y_pred = log_mod.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE Cross Validation Scores:  [0.85901639 0.87868852 0.9442623  0.93114754 0.86842105]\n",
      "FROST Cross Validation Scores:  [0.94444444 0.92929293 0.94949495 0.95454545 0.83333333]\n",
      "Average SMOTE CV Score:  0.896307161345988\n",
      "Average FROST CV Score:  0.9222222222222222\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale the features to [0, 1] range\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply SMOTE for oversampling\n",
    "smote = SMOTE()\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "# Define FROST function\n",
    "def generate_frost_samples(X_minority, initial_feature_index, k=5, m=1.5):\n",
    "    initial_feature_values = X_minority[:, initial_feature_index]\n",
    "    similarity_matrix = 1 / (\n",
    "        1 + np.abs(initial_feature_values[:, np.newaxis] - initial_feature_values)\n",
    "    )\n",
    "    k_nearest_indices = np.argsort(similarity_matrix, axis=1)[:, -k:]\n",
    "    synthetic_samples_initial = []\n",
    "    for i in range(len(initial_feature_values)):\n",
    "        for j in k_nearest_indices[i]:\n",
    "            synthetic_value = initial_feature_values[i] + m * (\n",
    "                initial_feature_values[j] - initial_feature_values[i]\n",
    "            )\n",
    "            synthetic_sample = np.copy(X_minority[i])\n",
    "            synthetic_sample[initial_feature_index] = synthetic_value\n",
    "            synthetic_samples_initial.append(synthetic_sample)\n",
    "    return np.array(synthetic_samples_initial)\n",
    "\n",
    "\n",
    "# Apply FROST for oversampling\n",
    "initial_feature_index = 0  # Choose the index of the initial feature to oversample\n",
    "X_train_frost = generate_frost_samples(\n",
    "    X_train_scaled[y_train == 1], initial_feature_index, k=5, m=1.5\n",
    ")\n",
    "\n",
    "# Combine original and synthetic samples\n",
    "X_train_combined = np.vstack((X_train_scaled, X_train_frost))\n",
    "y_train_combined = np.concatenate((y_train, np.ones(len(X_train_frost))))\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the number of folds for k-fold cross-validation\n",
    "k_folds = KFold(n_splits=5)\n",
    "\n",
    "# Perform cross-validation and calculate the scores for SMOTE\n",
    "scores_smote = cross_val_score(clf, X_train_smote, y_train_smote, cv=k_folds)\n",
    "\n",
    "# Perform cross-validation and calculate the scores for FROST\n",
    "scores_frost = cross_val_score(clf, X_train_combined, y_train_combined, cv=k_folds)\n",
    "\n",
    "# Print the cross-validation scores for each fold\n",
    "print(\"SMOTE Cross Validation Scores: \", scores_smote)\n",
    "print(\"FROST Cross Validation Scores: \", scores_frost)\n",
    "\n",
    "# Print the average cross-validation score\n",
    "print(\"Average SMOTE CV Score: \", scores_smote.mean())\n",
    "print(\"Average FROST CV Score: \", scores_frost.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using FROST, we have a higher score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Metrics: SMOTE\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.45      0.61       193\n",
      "           1       0.05      0.86      0.10         7\n",
      "\n",
      "    accuracy                           0.46       200\n",
      "   macro avg       0.52      0.65      0.36       200\n",
      "weighted avg       0.96      0.46      0.60       200\n",
      "\n",
      "[[ 86 107]\n",
      " [  1   6]]\n",
      "\n",
      "Model Evaluation Metrics: FROST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.45      0.61       193\n",
      "           1       0.05      0.86      0.10         7\n",
      "\n",
      "    accuracy                           0.46       200\n",
      "   macro avg       0.52      0.65      0.36       200\n",
      "weighted avg       0.96      0.46      0.60       200\n",
      "\n",
      "[[ 86 107]\n",
      " [  1   6]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nrutachoudhari/opt/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/nrutachoudhari/opt/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "log_mod = LogisticRegression()\n",
    "\n",
    "log_mod.fit(X_train_smote, y_train_smote)\n",
    "log_mod.fit(X_train_combined, y_train_combined)\n",
    "\n",
    "y_predSMOTE = log_mod.predict(X_test)\n",
    "y_predFROST = log_mod.predict(X_test)\n",
    "\n",
    "print(\"Model Evaluation Metrics: SMOTE\")\n",
    "print(classification_report(y_test, y_predSMOTE))\n",
    "print(confusion_matrix(y_test, y_predSMOTE))\n",
    "\n",
    "print(\"\\nModel Evaluation Metrics: FROST\")\n",
    "print(classification_report(y_test, y_predFROST))\n",
    "print(confusion_matrix(y_test, y_predFROST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Model Evaluation Metrics on Resampled Data- SMOTE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.59      0.62       762\n",
      "           1       0.62      0.68      0.65       762\n",
      "\n",
      "    accuracy                           0.63      1524\n",
      "   macro avg       0.64      0.63      0.63      1524\n",
      "weighted avg       0.64      0.63      0.63      1524\n",
      "\n",
      "[[447 315]\n",
      " [242 520]]\n"
     ]
    }
   ],
   "source": [
    "# Define a range of hyperparameters to search\n",
    "param_grid = {\n",
    "    \"penalty\": [\"l1\", \"l2\"],  # Regularization type\n",
    "    \"C\": np.logspace(\n",
    "        -3, 3, 7\n",
    "    ),  # Inverse of regularization strength (smaller values for stronger regularization)\n",
    "    \"solver\": [\"liblinear\"],  # Solver for l1 regularization\n",
    "}\n",
    "\n",
    "# Create a grid search with cross-validation\n",
    "grid_search = GridSearchCV(log_mod, param_grid, cv=5, scoring=\"f1\", n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Get the best hyperparameters and corresponding model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Evaluate the best model on the resampled data\n",
    "y_pred = best_model.predict(X_train_smote)\n",
    "\n",
    "print(\"Model Evaluation Metrics on Resampled Data- SMOTE:\")\n",
    "print(classification_report(y_train_smote, y_pred))\n",
    "print(confusion_matrix(y_train_smote, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with FROST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 10.0, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Model Evaluation Metrics on Resampled Data- FROST:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.99      0.87       762\n",
      "         1.0       0.57      0.05      0.10       228\n",
      "\n",
      "    accuracy                           0.77       990\n",
      "   macro avg       0.67      0.52      0.48       990\n",
      "weighted avg       0.73      0.77      0.69       990\n",
      "\n",
      "[[753   9]\n",
      " [216  12]]\n"
     ]
    }
   ],
   "source": [
    "# Define a range of hyperparameters to search\n",
    "param_grid = {\n",
    "    \"penalty\": [\"l1\", \"l2\"],  # Regularization type\n",
    "    \"C\": np.logspace(\n",
    "        -3, 3, 7\n",
    "    ),  # Inverse of regularization strength (smaller values for stronger regularization)\n",
    "    \"solver\": [\"liblinear\"],  # Solver for l1 regularization\n",
    "}\n",
    "\n",
    "# Create a grid search with cross-validation\n",
    "grid_search = GridSearchCV(log_mod, param_grid, cv=5, scoring=\"f1\", n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_combined, y_train_combined)\n",
    "\n",
    "# Get the best hyperparameters and corresponding model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Evaluate the best model on the resampled data\n",
    "y_pred = best_model.predict(X_train_combined)\n",
    "\n",
    "print(\"Model Evaluation Metrics on Resampled Data- FROST:\")\n",
    "print(classification_report(y_train_combined, y_pred))\n",
    "print(confusion_matrix(y_train_combined, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating with SMOTE for different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Decision Tree Classifier:\n",
      "Accuracy: 0.9311475409836065\n",
      "Precision: 0.9161676646706587\n",
      "Recall: 0.95625\n",
      "F1 Score: 0.9357798165137615\n",
      "Confusion Matrix: [[131  14]\n",
      " [  7 153]]\n",
      "\n",
      "Results for Random Forest Classifier:\n",
      "Accuracy: 0.9704918032786886\n",
      "Precision: 0.9575757575757575\n",
      "Recall: 0.9875\n",
      "F1 Score: 0.9723076923076923\n",
      "Confusion Matrix: [[138   7]\n",
      " [  2 158]]\n",
      "\n",
      "Results for Support Vector Machine (SVM):\n",
      "Accuracy: 0.9245901639344263\n",
      "Precision: 0.8743169398907104\n",
      "Recall: 1.0\n",
      "F1 Score: 0.9329446064139941\n",
      "Confusion Matrix: [[122  23]\n",
      " [  0 160]]\n",
      "\n",
      "Results for K-Nearest Neighbors (KNN):\n",
      "Accuracy: 0.839344262295082\n",
      "Precision: 0.7655502392344498\n",
      "Recall: 1.0\n",
      "F1 Score: 0.8672086720867209\n",
      "Confusion Matrix: [[ 96  49]\n",
      " [  0 160]]\n",
      "\n",
      "Results for Gradient Boosting Classifier:\n",
      "Accuracy: 0.9311475409836065\n",
      "Precision: 0.9483870967741935\n",
      "Recall: 0.91875\n",
      "F1 Score: 0.9333333333333333\n",
      "Confusion Matrix: [[137   8]\n",
      " [ 13 147]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_classification_models(X_train_smote, y_train_smote):\n",
    "    # Split the resampled data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_train_smote, y_train_smote, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Define a dictionary of classification models\n",
    "    models = {\n",
    "        \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
    "        \"Random Forest Classifier\": RandomForestClassifier(),\n",
    "        \"Support Vector Machine (SVM)\": SVC(),\n",
    "        \"K-Nearest Neighbors (KNN)\": KNeighborsClassifier(),\n",
    "        \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate and store various metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        results[model_name] = {\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1,\n",
    "            \"Confusion Matrix\": confusion,\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "results = evaluate_classification_models(X_train_smote, y_train_smote)\n",
    "for model_name, model_result in results.items():\n",
    "    print(f\"Results for {model_name}:\")\n",
    "    for metric, value in model_result.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating using FROST for different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Decision Tree Classifier:\n",
      "Accuracy: 0.9343434343434344\n",
      "Precision: 0.803921568627451\n",
      "Recall: 0.9318181818181818\n",
      "F1 Score: 0.8631578947368421\n",
      "Confusion Matrix: [[144  10]\n",
      " [  3  41]]\n",
      "\n",
      "Results for Random Forest Classifier:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Confusion Matrix: [[154   0]\n",
      " [  0  44]]\n",
      "\n",
      "Results for Support Vector Machine (SVM):\n",
      "Accuracy: 0.8484848484848485\n",
      "Precision: 0.85\n",
      "Recall: 0.38636363636363635\n",
      "F1 Score: 0.53125\n",
      "Confusion Matrix: [[151   3]\n",
      " [ 27  17]]\n",
      "\n",
      "Results for K-Nearest Neighbors (KNN):\n",
      "Accuracy: 0.8737373737373737\n",
      "Precision: 0.6376811594202898\n",
      "Recall: 1.0\n",
      "F1 Score: 0.7787610619469026\n",
      "Confusion Matrix: [[129  25]\n",
      " [  0  44]]\n",
      "\n",
      "Results for Gradient Boosting Classifier:\n",
      "Accuracy: 0.9696969696969697\n",
      "Precision: 0.9318181818181818\n",
      "Recall: 0.9318181818181818\n",
      "F1 Score: 0.9318181818181818\n",
      "Confusion Matrix: [[151   3]\n",
      " [  3  41]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_classification_models(X_train_combined, y_train_combined):\n",
    "    # Split the resampled data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_train_combined, y_train_combined, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Define a dictionary of classification models\n",
    "    models = {\n",
    "        \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
    "        \"Random Forest Classifier\": RandomForestClassifier(),\n",
    "        \"Support Vector Machine (SVM)\": SVC(),\n",
    "        \"K-Nearest Neighbors (KNN)\": KNeighborsClassifier(),\n",
    "        \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate and store various metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        results[model_name] = {\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1,\n",
    "            \"Confusion Matrix\": confusion,\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "results = evaluate_classification_models(X_train_combined, y_train_combined)\n",
    "for model_name, model_result in results.items():\n",
    "    print(f\"Results for {model_name}:\")\n",
    "    for metric, value in model_result.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Random Forest Classifier\n",
    "2. Gradient Boosting Algorithm\n",
    "3. Decision Tree Classifier\n",
    "4. K-Nearest Neighbors\n",
    "5. Support Vector Machine\n",
    "6. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Best Model Evaluation Metrics:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Confusion Matrix:\n",
      "[[154   0]\n",
      " [  0  44]]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_combined, y_train_combined, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the Random Forest Classifier model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define a range of hyperparameters to search\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 150],  # Number of trees in the forest\n",
    "    \"max_depth\": [None, 10, 20, 30],  # Maximum depth of the trees\n",
    "    \"min_samples_split\": [\n",
    "        2,\n",
    "        5,\n",
    "        10,\n",
    "    ],  # Minimum number of samples required to split an internal node\n",
    "    \"min_samples_leaf\": [\n",
    "        1,\n",
    "        2,\n",
    "        4,\n",
    "    ],  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Create a grid search with cross-validation\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring=\"f1\", n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the resampled data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and corresponding model\n",
    "best_params = grid_search.best_params_\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the best model on the training data\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# Calculate and print various metrics to evaluate the best model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Best Model Evaluation Metrics:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'ml_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "# saving it as a .pkl file\n",
    "\n",
    "with open(\"ml_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(best_rf_model, file)\n",
    "\n",
    "print(\"Model saved as 'ml_model.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
